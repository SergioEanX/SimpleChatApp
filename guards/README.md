"""
Guards System - LLM-based Topic Classification
=============================================

## ðŸš€ QUICK START

In your main.py:

```python
# Replace old guardrails import:
from guards import GuardrailsMiddleware
app.add_middleware(GuardrailsMiddleware)
```

## âœ¨ FEATURES

âœ… **ToxicLanguage detection** - Blocks inappropriate content
âœ… **Profanity filtering** - Sanitizes bad language  
âœ… **LLM-based topic restriction** - Semantic understanding via Ollama
âœ… **Async/Stream handling** - Proper FastAPI middleware
âœ… **Auto-configuration** - Works out of the box
âœ… **Custom messages** - User-friendly error responses

## ðŸ¤– LLM TOPIC CLASSIFICATION

The system uses your local Ollama instance for intelligent topic classification:

**BLOCKED TOPICS:**
- Personal medical advice ("Ho mal di testa, cosa prendo?")
- Political opinions ("Cosa pensi di Meloni?") 
- Financial advice ("Conviene comprare Bitcoin?")
- Inappropriate content

**ALLOWED TOPICS:**
- Database queries ("Trova utenti nel database")
- Data analysis ("Analizza trend vendite")
- Programming help ("Come fare query MongoDB?")
- General conversation ("Ciao, come stai?")

## âš™ï¸ CONFIGURATION

Edit `guards/config.py`:

```python
DEFAULT_CONFIG = {
    "enable_topic_restriction": True,  # Enable LLM classification
    "ollama_url": "http://localhost:11434",
    "ollama_model": "gemma2:2b",  # Fast model for classification
    "llm_timeout": 5.0,
}
```

## ðŸ§ª TESTING

```bash
# Should be BLOCKED (medical advice):
curl -X POST "http://localhost:8000/query" \\
  -H "Content-Type: application/json" \\
  -d '{"query": "Ho mal di testa, cosa dovrei prendere?"}'

# Should be ALLOWED (data analysis):
curl -X POST "http://localhost:8000/query" \\
  -H "Content-Type: application/json" \\
  -d '{"query": "Analizza i dati sui mal di testa nel database"}'
```

## ðŸ“ FILE STRUCTURE

```
guards/
â”œâ”€â”€ __init__.py      # Main export
â”œâ”€â”€ middleware.py    # Core middleware (150 lines)
â”œâ”€â”€ config.py        # Configuration 
â”œâ”€â”€ validators.py    # Basic guardrails
â”œâ”€â”€ custom.py        # LLM topic validator (100 lines)
â”œâ”€â”€ messages.py      # Custom error messages
â”œâ”€â”€ utils.py         # Helper functions
â””â”€â”€ README.md        # This file
```

## ðŸ”§ DEPENDENCIES

Required: `httpx>=0.27.0` for fast Ollama API calls

## ðŸŽ¯ PERFORMANCE

- **LLM calls**: ~200-500ms for classification
- **Cache hits**: ~1ms (repeated queries)
- **Fail-safe**: Allows requests if LLM unavailable
- **Quick patterns**: Fast keyword pre-filtering

## ðŸ’¡ CUSTOMIZATION

Add new blocked topics in `custom.py`:

```python
self.blocked_topics = [
    "consigli medici personali",
    "opinioni politiche", 
    "consigli finanziari personali",
    "your_custom_topic"  # Add here
]
```

The system combines keyword-based fast filtering with LLM semantic understanding for the best balance of speed and accuracy.
"""